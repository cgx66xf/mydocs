ls-Major marchine learning techniques:
	
	Regression/Estimation
		Usefull for predicting continious values ex:
		Price of a houseestimating or the CO2 emissions of an engine
	
	Classification
		Predicting the item class/category of a case
		Ex:Classifying if a cell is benine or malignant
	
	Clustering
		Finding the structure of data; summarization
		Example of clustering is finding similar patients or customer segmentation
		
	Associations
		Associating frequent co-occuring events/items
		Example of associations would be grocery items that are bought together by a customer
		
	Anomaly Detection
		Discovering anormal or absurd cases
		Ex: Credit card fraud detection
	
	Sequence mining
		Predicting the next events; Clickstream in websites
		
	Dimension Reduction
		Used for reducing the size of data
	
	Reccomendation Systems
	 	Associates peoples preferances to similar people and gives recommendations depending on that

-------------------------------------------------------------------------------------------------------------------
Machine learning is the branch of ai that covers the statistical part of ai, it teaches the computer solving of problems by looking at hundereds or thousands of examples learning from them and than using that exprience to solve the same problem in new situations

Deep learning is a special field of machine learning where computers can actually learn and make intelligent decisions on their own, deep learning involves a deeper level of automation in comparison with most ml algorithms
---------------------------Python for Ml----------------------
NumPy is a python library for working with N dimensional arrays
Scipy is a collection of numerical algorithms and domain specific toolboxes scipy is a good library for high performance scientific programming
Matplotlib is a library for 2d and 3d plotting
Pandas is a high level python library that provides high performance easy to use data structures

Scikitlearn is a collection of algorithms and tools for machine learning
It has most of the classification, Regression and Clustering algorithms 
Works with numpy and scipy

Machine learning algorithms benefit from standardization of the dataset if there is some outliers or diffrent scales in our dataset we have to change them
The preprocessing package from sckikit learn provides several utility functions and transformer classes to change raw feature vectors into a suitable form  of vector for modeling

You have to split your dataset into train and test sets to train your model and then test the models accuracy seperately 
Scikitlearn can split arrays or matrices into random train and test subsets for you in one line of code, than you can set up your algorithm
for example: You can build a classifier using a support vector classification algorithm
-------------------------Supervised vs Unsupervised----------------------
How do we teach a model:
We teach a model by feeding it with a labeled dataset 
There are 2 types of supervised learning: Classification and Regression
Classification is the process of predicting discrete class labels or categories 
Regression is the process of predicting continuous values

In unsupervised learning we let the model discover information on its own that may be invisible to human eye
Unsupervised algorithms draws conclusions on unlabeled data
Unsupervised algorithms are more difficult than supervised 
Unsupervised learning techniques:
	Dimension Reduction
	Density estimation
	Market basket analysis
	Clustering
	
Supervised                 |     Unuspervised
-Classification                  -Clustering
classifies labeled data          Finds patterns and groupings from unlabeled data
-Regression                      

-----------------------------------------Regression----------------------
In regression there is 2 types of variables
A dependent variable X and one or more independent variables
Dependent variable Y can be seen as the target we are trying to predict
A regression model relates y to a function of x 
The key point of regression is the dependent value should be continious and cannot be discrete however the independent variable can be measured in a categorical value or a continous scale

Two types of regression models:
	-Simple Regression:
		Simple Linear Regression:     Predicting co2 emmision vs Engine size
		Simple Non-Linear Regression
	 Simple regression is when an independent value is used to predict an dependent value 
	-Multiple Regression:
	When more than one indpendent variable is used to predict the dependent variable its multiple regression
		Multiple linear regression     Predicting co2 emission vs Engine size and Cylinders 
		Multiple non-linear regression

Regression Algorithms
-Ordinal regression, -Poisson regression, -Fast forest quantile regression
-Linear, Polynomial, Lasso, Stepwise, Ridge regression
-Bayessian linear regression, -Neural network regression, -Decision forest regression
-Boosted decision tree regression, -KNN(K-nearest neighbors)regression


---Simple linear regression:
In simple linear regression we fit a line in the graph
y= i+jx we represent the line as a polynomial   j is the slope and i is the intercept
Now the question is how do we draw a line through the point and which line fits best

How to find the best fitting line in simple linear regression:
Error= y(prediction)-y(actual)   #this is also called residual error
     = 250 - 340 
     = -90
Error is the disctance from the datapoint to the fitted regression line
The mean of all residual errors shows how poorly the line fits with the whole dataset
The mathmetical represantation of this:
	        1  n 
           MSE=	-  |  (y(prediction)-y(actual))^2
                n  i=1
# | is a summation notation
The objective of linear regression is to minimise the MSE(Mean squared error) to do that we should find the best parameters i and j for the equation y=i+jx
There is 2 options to do that: Mathematical approach and Optimization Approach

Mathematical approach to do this:
Given that its a simple linear regression with only one independent variable y= i+jx given that i is the intercept and j is the slope of the line, we can estimate them directly from our data
1.It requires that we calculate the mean of the independent and dependent columns from the dataset 
y= (196+221+136+..)/9 = 226.22
x= (2+2.4+1.5+ ...)/9 = 3.03 te the slope equation here)
3. After we find the j(slope) we can plug it into the line equation to find the i(intercept)

Pros of linear regression:
	Very fast
	No parameter tuning
	Easy to understand and highly interpartable 
-----------Model evaluation in Regression models-------------
The goal of regression is tobuild a model to predict an unknown case, to this end we have to perform regression evaluation after building the model we will now discuss the two types of evaluation approaches
-Train and test on the same dataset
-Train/Test split
How can we calculate the accuaracy of our model?
One of the solution is to select a portion of our dataset for testing for example we have 10 records in our dataset we use the entire dataset for training and we build a model using this training set.
-1 Lets select a small portion of the dataset such as row number six to nine but without the labels, this is called a test set the labels are called actual values of the test set 
and we pass the parameters of the test set to our model and compare it to the predicted values.
This is called Train and Test on the same dataset

Training and testing on the same dataset produces a high training accuracy; training accuracy is the percantage of correct predictions that the model makes when using the test dataset however a high training accuracy isnt necessarrily a good thing for instance having a high training accuracy may result in an over-fit of the data this means that the model is overly trained to the dataset whichmay capture noise and produce a non-generalized model.
Out-of-sample accuracy is the percentage of correct perdictions that the model makes on data that the model has not been trained on. Foing a train and test on the same dataset will most likely have low out-out-sample accuracy due to the likelihood of being over-fit its important that our models have high out of sample accuracy because we want to make correct predictions on unknown data.
So how can we improve out-of-sample accuracy? One way is to use another evaluation train/test split

In train/test split we use a portion of our dataset for training and the rest is used for testing.
+ More accurate evaluation on out-of-sample accuracy 
- Highly dependent on which datasets the data is trained and tested
Another evalutaion model called K-fold cross validation, resolves most of these issues
-----Evaluation metrics in regression models-------
We will be reviewing a number of model evalitoion metrics including:Mean absolute error, Mean squared error, Root Mean squared error
But before that we need to define what error really is, in the context of regression the error of the model is the deffrence between the data points and the trend line generated by the algorithm
Since there are multiple data points an error can be determined in multiple ways:
Mean absolute error: is the mean of the absolute values of the erros: MAE= 1/n*summation(1 to n)|y-y| this is the easiest of the metrics to understand
Mean squared error: is the mean of the squared error. Its more popular than Mean absolute error because the focus is geared more towards learge errors this is due to the squared term, exponentially increasing large errors in comparison to the small ones: MSE= 1/n*summation((y-y)^2)
Root mean squared error: Is the the square root of the mean squared error:MSE^^1/2  This is one of the popular evaluation metrics because root means squared error is interpartable as the same units as the y units 
Relative absolute error:
Relative sqared error: Used to calculate R squared and used by the data scientist
R^= 1- RSE and represents how close the data values are to the fitted regression line the higher the r squared the better the model fits your data

----------------------Multiple Linear Regression-----------------------
2 use cases for multiple linear regression
1-Independent variables effectiveness on prediction: Does gender and age have impacts on exam performance
2-Predicting impacts of changes: How much does blood pressure change for unic change in body mass index

Predicting continious values with multiple regression:
Generally multiple regression model is of the form:  y= 0+ 0*x+ 0*x + ... + 0n*xn
We can show it as a vector form as well:  y= 0^T X  the dot products of the parameters vector and the feature set vector 

Estimating the paramater 0(theta) : 
	Ordinary least squares:
		Ordinary least squares tries to estimate the values of the co-efficents by minimizing the MeanSquareError(MSE) 
		This approach uses the data as a matrix and uses linear algebra operations to estimate the optimal values for theta
		Takes a long time(+10k rows)  
	Optimization algorithm methods:
		1-Gradient descent: Starts optimization with random values for each co-efficent, than calculates the errors and tries to minimize it by changing the co-efficents in multiple iterations gradient descent is the proper 		approach if you have a large dataset
		2-There is also other coefficent approximation methods
		
After we find the parameters of the linear regression making predictions is as simple as solving the equation for a specific set of inputs 
y= 0^T X   0^T= [125, 6.2, 14, ...] so y= 125+ 6.2x+ 14x+ ... so CO2EM= 125+ 6.2ENGINESIZE+ 14CYLINDERS+ ...

Adding too many independent variables without any theoretical justification may result in a overfit model
Categorical independent variables can be incorperated into a regression model by converting them into numerical variables, for example a 0 for manual cars and 1 for automatic cars 

Note: Remember that multiple linear regression is a linear type of regression so there needs to be a linear relationship between each of the independent variables and dependent variable.
There are multiple ways to check for linearity for example you can use scatter plots and then visually check for linearity, if the relationship displayed in your scatterplot is not linear than we need to use non-linear regression


	
